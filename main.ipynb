{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART1: Load AraGPT2\n",
    "\n",
    "Using the link below, learn how to load araGPT2 base model.\n",
    "\n",
    "https://huggingface.co/aubmindlab/aragpt2-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arabert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, pipeline\n",
    "from transformers import GPT2LMHeadModel\n",
    "from arabert.aragpt2.grover.modeling_gpt2 import GPT2LMHeadModel\n",
    "from arabert.preprocess import ArabertPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Complete this cell\n",
    "MODEL_NAME=\n",
    "arabert_prep =\n",
    "\n",
    "text=\"الجزائر بلد\"\n",
    "text_clean = arabert_prep.preprocess(text)\n",
    "\n",
    "model =\n",
    "tokenizer =\n",
    "generation_pipeline =\n",
    "\n",
    "#feel free to try different decoding settings\n",
    "generation_pipeline(text,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    num_beams=10,\n",
    "    max_length=200,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty = 3.0,\n",
    "    no_repeat_ngram_size = 3)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print AraGPT Model and analyze the architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: print AraGPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART2: Fine-tuning\n",
    "\n",
    "To fine-tune AraGPT2 for text summarization, we use the file `arabic_texts_summaries.csv`\n",
    "\n",
    "#### *Fine-tuning Steps:*\n",
    "\n",
    "\n",
    "1.   Load datasets and split it into train/test\n",
    "2.   Create Datalaoders of train and val.\n",
    "3.   Resize model embeddings for new tokenizer length.\n",
    "4.   Fine-tuning model by passing train data and evaluating it on val data during training.\n",
    "5.   Store the tokenizer and fine-tuned model.\n",
    "6.   Generate summaries for test set which is not used during fine tune.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils_data import * \n",
    "from src.utils_tokenizer import * \n",
    "from src.train import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = \n",
    "sum_length = \n",
    "split_probability = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = process_data(\"data/arabic_texts_summaries.csv\",max_length , sum_length, split_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add token to AraGPT2 tokenizer \n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('aubmindlab/aragpt2-base')\n",
    "\n",
    "special_tokens = {'bos_token':'<BOS>', 'eos_token':'<EOS>', 'pad_token':'<PAD>', 'additional_special_tokens':['<SUMMARIZE>']}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "print('tokenizer len: {}'.format(len(tokenizer)))\n",
    "\t\n",
    "ignore_idx = tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: apply tokenizer \n",
    "import os \n",
    "\n",
    "tokenizer_dir =\"tokenizer_path_save\"\n",
    "if not os.path.exists(tokenizer_dir):\n",
    "  os.makedirs(tokenizer_dir) # Create output directory if needed\n",
    "\n",
    "max_seq_len = 768 \n",
    "tokenizer.save_pretrained(tokenizer_dir)\n",
    "tokenizer_len = len(tokenizer)\n",
    "print('ignore_index: {}'.format(ignore_idx))\n",
    "print('max_len: {}'.format(max_seq_len))\n",
    "\n",
    "train, val, test = # Fix tokenize_dataset function in utils_tokenizer and call it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate train/val/test files\n",
    "#save tokenized data\n",
    "out_dir=\"tokenizer_data\"\n",
    "processed_set= \"dataset\"\n",
    "data_dir = os.path.join(out_dir, processed_set)\n",
    "if not os.path.exists(data_dir):\n",
    "  os.makedirs(data_dir) # Create output directory if needed\n",
    "file = os.path.join(data_dir,\"train.csv\")\n",
    "train.to_csv(file, index=False)\n",
    "\n",
    "file = os.path.join(data_dir,\"val.csv\")\n",
    "val.to_csv(file, index=False)\n",
    "\n",
    "file = os.path.join(data_dir,\"test.csv\")\n",
    "test.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize train and explain each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Data Loaders \n",
    "# Fix code in utils_data.py \n",
    "\n",
    "import torch\n",
    "train_dataset, val_dataset= # call function get_gpt2_dataset\n",
    "\t\n",
    "b = train_dataset.__getitem__() # check one data row\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = 1)\n",
    "val_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset), batch_size = 1)\n",
    "\n",
    "train_loader_len = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"out_dir\" : \n",
    "    \"training_models\":\n",
    "    \"final_model\": \n",
    "}\n",
    "\n",
    "# fine tune pretrained model \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dir =  'aubmindlab/aragpt2-base'\n",
    "\n",
    "train = Train(device, model_dir, tokenizer_len, ignore_idx, train_loader_len, config)\n",
    "train.train_model(train_dataloader, val_dataloader)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
